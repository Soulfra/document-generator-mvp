# Docker Compose for AI API Service
# Can be used standalone or integrated with main docker-compose.yml

version: '3.8'

services:
  ai-api:
    build:
      context: .
      dockerfile: src/services/ai-api/Dockerfile
    container_name: finishthisidea-ai-api
    restart: unless-stopped
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=production
      - AI_API_PORT=3001
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      
      # LLM Provider Settings
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      
      # Internal Authentication
      - INTERNAL_API_KEY=${INTERNAL_API_KEY}
      
      # Rate Limiting
      - RATE_LIMIT_WINDOW_MS=60000
      - RATE_LIMIT_MAX_REQUESTS=1000
      
      # Cost Controls
      - MAX_COST_PER_REQUEST=5.00
      - MAX_COST_PER_DAY=500.00
      
      # Features
      - ENABLE_BYOK=true
      - ENABLE_CUSTOM_MODELS=true
      
    volumes:
      - ai-logs:/usr/src/app/logs
      - ./config:/usr/src/app/config:ro
    
    networks:
      - finishthisidea-network
    
    depends_on:
      - postgres
      - redis
      - ollama
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama service for local LLM processing
  ollama:
    image: ollama/ollama:latest
    container_name: finishthisidea-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - finishthisidea-network
    
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # PostgreSQL for persistent data
  postgres:
    image: postgres:15-alpine
    container_name: finishthisidea-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=finishthisidea
      - POSTGRES_USER=finishthisidea
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./prisma/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "5432:5432"
    networks:
      - finishthisidea-network

  # Redis for caching and rate limiting
  redis:
    image: redis:7-alpine
    container_name: finishthisidea-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"
    networks:
      - finishthisidea-network

  # Analytics Dashboard (optional)
  analytics:
    build:
      context: .
      dockerfile: src/monitoring/Dockerfile
    container_name: finishthisidea-analytics
    restart: unless-stopped
    ports:
      - "8889:8889"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=${DATABASE_URL}
      - ANALYTICS_PORT=8889
    networks:
      - finishthisidea-network
    depends_on:
      - postgres

volumes:
  ai-logs:
    driver: local
  ollama-data:
    driver: local
  postgres-data:
    driver: local
  redis-data:
    driver: local

networks:
  finishthisidea-network:
    driver: bridge